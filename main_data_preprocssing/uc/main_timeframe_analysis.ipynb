{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/Documents/Diffusion_motion_field/denoising_diffusion_pytorch/denoising_diffusion_pytorch/conditional_diffusion_3D.py:882: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Documents')\n",
    "# imports\n",
    "import os, sys\n",
    "\n",
    "# third party imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import nibabel as nb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import Diffusion_motion_field.Build_lists.Build_list as Build_list\n",
    "import Diffusion_motion_field.functions_collection as ff\n",
    "import Diffusion_motion_field.Data_processing as Data_processing\n",
    "\n",
    "from Diffusion_motion_field.denoising_diffusion_pytorch.denoising_diffusion_pytorch.conditional_EDM_warp import *\n",
    "\n",
    "main_path = '/mnt/camca_NAS/4DCT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the patient list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317, 12)\n"
     ]
    }
   ],
   "source": [
    "patient_list = pd.read_excel(os.path.join(main_path,'Patient_lists/uc/patient_list_MVF_diffusion_train_test.xlsx'))\n",
    "print(patient_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 CVC1807171636 Abnormal\n",
      "283 CVC2006261457 Abnormal\n",
      "284 CVC1811270917 Normal\n",
      "285 CVC1805310959 Normal\n",
      "286 CVC1910150845 Abnormal\n",
      "287 CVC1908301036 Normal\n",
      "288 CVC1801031507 Normal\n",
      "289 CVC1912181517 Abnormal\n",
      "290 CVC1907301359 Abnormal\n",
      "291 CVC1912160957 Normal\n",
      "292 CVC1802051130 Abnormal\n",
      "293 CVC2005201013 Abnormal\n",
      "294 CVC1811191453 Abnormal\n",
      "295 CVC1904240910 Normal\n",
      "296 CVC2006041408 Normal\n",
      "297 CVC2001081016 Abnormal\n",
      "298 CVC2006021121 Abnormal\n",
      "299 CVC1910221524 Normal\n",
      "300 CVC2001280905 Abnormal\n",
      "301 CVC2006250902 Normal\n",
      "302 CVC1912111149 Normal\n",
      "303 CVC2002250842 Normal\n",
      "304 CVC1812271121 Normal\n",
      "305 CVC1905311311 Abnormal\n",
      "306 CVC2002131112 Abnormal\n",
      "307 CVC1911050924 Normal\n",
      "308 CVC2005201443 Normal\n",
      "309 CVC1901111110 Normal\n",
      "310 CVC1912121107 Abnormal\n",
      "311 CVC1908061347 Abnormal\n",
      "312 CVC1907251059 Normal\n",
      "313 CVC2004131548 Abnormal\n",
      "314 CVC2003191457 Abnormal\n",
      "315 CVC1905071046 Normal\n",
      "316 CVC1902151307 Normal\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(282,patient_list.shape[0]):\n",
    "    patient_id = patient_list['patient_id'][i]\n",
    "    patient_class = patient_list['patient_class'][i]\n",
    "    print(i, patient_id, patient_class)\n",
    "\n",
    "    seg_folder = os.path.join(main_path, 'predicted_seg', patient_class, patient_id,'seg-pred-0.625-4classes-connected-retouch-resampled-1.5mm')\n",
    "\n",
    "    seg_files = ff.sort_timeframe(ff.find_all_target_files(['pred*'],seg_folder),2,'_')\n",
    "\n",
    "    total_tf_num = len(seg_files) \n",
    "    \n",
    "    # get the list of LV volume\n",
    "    LV_volume_list = []\n",
    "    for tf in range(total_tf_num):\n",
    "        seg_file = seg_files[tf]\n",
    "        seg_data = nb.load(seg_file).get_fdata(); seg_data = seg_data.astype(np.int16)\n",
    "        LV_volume = np.sum(seg_data == 1)\n",
    "        LV_volume_list.append(LV_volume)\n",
    "    LV_volume_list = np.asarray(LV_volume_list)\n",
    "\n",
    "    # find the index of the minimum volume as well as the Ejection fraction\n",
    "    es_index = np.argmin(LV_volume_list)\n",
    "    ejection_fraction = (LV_volume_list[0] - LV_volume_list[es_index])/LV_volume_list[0]\n",
    "    last_tf_percent = (LV_volume_list[0] - LV_volume_list[-1])/LV_volume_list[0]\n",
    "\n",
    "    # turn the time frame list into [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    # first find out the normalized index of es\n",
    "    es_index_normalized = round(es_index/(total_tf_num),1)\n",
    "\n",
    "    # sample the temporal series\n",
    "    normalized_time_frame_list = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]; normalized_time_frame_list_copy = normalized_time_frame_list.copy()\n",
    "    sampled_time_frame_list = []\n",
    "    for t in range(0,len(normalized_time_frame_list)):\n",
    "        normalized_time_frame = normalized_time_frame_list[t]\n",
    "        if normalized_time_frame == es_index_normalized:\n",
    "            sampled_time_frame_list.append(es_index)\n",
    "        else:\n",
    "            time_index = round(normalized_time_frame*total_tf_num)\n",
    "            if time_index == es_index or time_index in sampled_time_frame_list:\n",
    "                # remove this in normalized_time_frame_list\n",
    "                normalized_time_frame_list_copy.remove(normalized_time_frame)\n",
    "                continue\n",
    "            assert time_index != es_index # make sure the sampled time frame is not the ES time frame\n",
    "            sampled_time_frame_list.append(time_index)\n",
    "\n",
    "    # also calculate if pick [0.1,0.3,0.5,0.7,0.9], the ejection fraction is?\n",
    "    # calculate the ejection fraction directly using semgnetaiton at each time frame\n",
    "    picked_tf_normalized = [0.1,0.3,0.5,0.7,0.9]\n",
    "    if len(sampled_time_frame_list)< 10:\n",
    "        ejection_fraction_picked_5tf = ''\n",
    "    else:\n",
    "        picked_tf = [sampled_time_frame_list[normalized_time_frame_list.index(picked_tf_normalized[iii])] for iii in range(0,len(picked_tf_normalized))]\n",
    "        picked_LV_volume_list = [LV_volume_list[picked_tf[iii]] for iii in range(0,len(picked_tf))]\n",
    "        ejection_fraction_picked_5tf_from_seg = (LV_volume_list[0] - np.min(picked_LV_volume_list))/LV_volume_list[0]\n",
    "    # print('volume list: ', picked_LV_volume_list)\n",
    "    # print('ejection_fraction_picked_5tf_from_seg: ', ejection_fraction_picked_5tf_from_seg)\n",
    "\n",
    "    # also calculate if pick [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], the ejection fraction is?\n",
    "    # calculate the ejection fraction directly using semgnetaiton at each time frame\n",
    "    picked_tf_normalized = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    if len(sampled_time_frame_list)< 10:\n",
    "        ejection_fraction_picked_10tf = ''\n",
    "    else:\n",
    "        picked_tf = [sampled_time_frame_list[normalized_time_frame_list.index(picked_tf_normalized[iii])] for iii in range(0,len(picked_tf_normalized))]\n",
    "        picked_LV_volume_list = [LV_volume_list[picked_tf[iii]] for iii in range(0,len(picked_tf))]\n",
    "        ejection_fraction_picked_10tf_from_seg = (LV_volume_list[0] - np.min(picked_LV_volume_list))/LV_volume_list[0]\n",
    "\n",
    "    # also calculate the ejection fraction using segmentation at ED 0 and deformation field at each time frame\n",
    "    if len(sampled_time_frame_list)< 10:\n",
    "        ejection_fraction_picked_5tf_from_mvf = ''\n",
    "        ejection_fraction_picked_10tf_from_mvf = ''\n",
    "    else:\n",
    "        picked_tf = [sampled_time_frame_list[normalized_time_frame_list.index(picked_tf_normalized[iii])] for iii in range(0,len(picked_tf_normalized))]\n",
    "        seg_template = nb.load(os.path.join(seg_folder, 'pred_s_0.nii.gz')).get_fdata()\n",
    "        seg_template = np.round(seg_template).astype(np.int16)  # make sure the seg_template is in int16 format\n",
    "        seg_template[seg_template != 1] = 0\n",
    "        seg_template = Data_processing.crop_or_pad(seg_template, [160,160,96], value = 0)\n",
    "\n",
    "        mvf_folder = os.path.join('/mnt/camca_NAS/4DCT/mvf_warp0_onecase/',patient_class, patient_id, 'voxel_final')\n",
    "        volume_list = []\n",
    "        volume_list = []\n",
    "        seg_template_torch = torch.from_numpy(seg_template).unsqueeze(0).unsqueeze(0).float().cuda()\n",
    "        for tf_n in range(0,len(picked_tf)):\n",
    "            mvf_file = os.path.join(mvf_folder, str(picked_tf[tf_n]) + '.nii.gz')\n",
    "            mvf_data = nb.load(mvf_file).get_fdata()\n",
    "            mvf_data_torch = torch.from_numpy(np.transpose(mvf_data, (3, 0, 1, 2))).unsqueeze(0).float().cuda()\n",
    "            warped_seg = warp_segmentation_from_mvf(seg_template_torch, mvf_data_torch)\n",
    "            warped_seg = warped_seg.squeeze(0).squeeze(0).cpu().numpy()\n",
    "            # warped_seg = Data_processing.apply_deformation_field_numpy(np.copy(seg_template), mvf_data)\n",
    "            volume_list.append(np.sum(warped_seg))\n",
    "        # print('volume_list: ', volume_list)\n",
    "        volume_list = np.asarray(volume_list)\n",
    "        ejection_fraction_picked_10tf_from_mvf = (LV_volume_list[0] - np.min(volume_list))/LV_volume_list[0]\n",
    "        volume_list_5tf = np.asarray([volume_list[1], volume_list[3], volume_list[5], volume_list[7], volume_list[9]]).reshape(-1)\n",
    "        ejection_fraction_picked_5tf_from_mvf = (LV_volume_list[0] - np.min(volume_list_5tf))/LV_volume_list[0]\n",
    "\n",
    "    # # print('Patient ID: ', patient_id, 'total time frame: ', total_tf_num, ' ES index: ', es_index, ' ES index normalized: ', es_index_normalized, ' sampled time frame: ', sampled_time_frame_list, ' normalized time frame: ', normalized_time_frame_list_copy)\n",
    "    # # print('ejection fraction original: ', np.round(ejection_fraction,3), 'ejection fraction sampled in 5 tf: ', np.round(ejection_fraction_picked_5tf,3), 'ejection fraction sampled in 10 tf: ', np.round(ejection_fraction_picked_10tf,3))\n",
    "        \n",
    "    # how to assert that there is no duplicate in sampled_time_frame_list\n",
    "    assert len(sampled_time_frame_list) == len(set(sampled_time_frame_list))\n",
    "    # turn LV_volume_list back to a list\n",
    "    LV_volume_list = LV_volume_list.tolist()\n",
    "\n",
    "    # # append the results to the results list\n",
    "    results.append([patient_class, patient_id, total_tf_num, es_index, es_index_normalized, sampled_time_frame_list, normalized_time_frame_list_copy, LV_volume_list, ejection_fraction, last_tf_percent, ejection_fraction_picked_5tf_from_seg, ejection_fraction_picked_5tf_from_mvf, ejection_fraction_picked_10tf_from_seg, ejection_fraction_picked_10tf_from_mvf])\n",
    "\n",
    "    df = pd.DataFrame(results, columns = ['patient_class', 'patient_id', 'total_tf_num', 'es_index', 'es_index_normalized', 'sampled_time_frame_list', 'normalized_time_frame_list_copy', 'LV_volume_list', 'EF_original', 'last_tf_percent', 'EF_sampled_in_5tf_by_seg', 'EF_sampled_in_5tf_by_mvf', 'EF_sampled_in_10tf_by_seg', 'EF_sampled_in_10tf_by_mvf'])\n",
    "    df.to_excel(os.path.join(main_path,'Patient_lists/uc/patient_list_final_selection_timeframes.xlsx'), index = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### according to timeframe, we shall exclude several cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list = pd.read_excel(os.path.join(main_path,'Patient_lists/patient_list_MVF_diffusion_train_test.xlsx'))\n",
    "timeframe_info = pd.read_excel(os.path.join(main_path,'Patient_lists/patient_list_final_selection_timeframes.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 12)\n"
     ]
    }
   ],
   "source": [
    "exclude_index = []\n",
    "for i in range(0,patient_list.shape[0]):\n",
    "    exclude = False\n",
    "    patient_id = patient_list['patient_id'][i]\n",
    "    row = timeframe_info[timeframe_info['patient_id'] == patient_id]\n",
    "\n",
    "    es_index_normalized = row['es_index_normalized'].iloc[0]\n",
    "\n",
    "    last_tf_percent = abs(row['last_tf_percent'].iloc[0])\n",
    "\n",
    "    if es_index_normalized < 0.3 or es_index_normalized >= 0.7:\n",
    "        exclude = True\n",
    "\n",
    "    if last_tf_percent >= 0.30:\n",
    "        exclude = True\n",
    "    \n",
    "    if exclude:\n",
    "        exclude_index.append(i)\n",
    "\n",
    "# remove the exclude index from the patient list\n",
    "patient_list = patient_list.drop(exclude_index)\n",
    "print(patient_list.shape)\n",
    "patient_list.to_excel(os.path.join(main_path,'Patient_lists/patient_list_MVF_diffusion_train_test_filtered.xlsx'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
