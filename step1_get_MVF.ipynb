{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "You should prepare two things before running this step. Please refer to the `example_data` folder for guidance:\n",
    "\n",
    "1. **NIfTI images** of 4DCT resampled to a voxel size of **[1.5, 1.5, 1.5] mmÂ³**.  \n",
    "   - Each cardiac phase should be saved as a separate file.  \n",
    "   - All files should be placed in a folder named:  \n",
    "     `img-nii-resampled-1.5mm`.\n",
    "\n",
    "2. **A patient list** that enumerates all your cases.  \n",
    "   - To understand the expected format, please refer to the file:  \n",
    "     `example_data/Patient_lists/example_data/patient_list.xlsx`.\n",
    "   - Make sure the number of time frames is equal to the number of nii files\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Get reference MVF for each 4DCT case\n",
    "\n",
    "In this script, we use Voxelmorph to obtain **motion vector field (MVF)** for each cardiac phase (time frame), using the first phase (end-diastole) as template\n",
    "\n",
    "---\n",
    "\n",
    "### Docker environment\n",
    "1. Please use `docker/docker_tensorflow`, it will build a tensorflow docker\n",
    "2. make sure you have `voxelmorph` installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 14:51:09.872624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os, sys\n",
    "sys.path.append('/workspace/Documents')\n",
    "\n",
    "# third party imports\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "import voxelmorph as vxm\n",
    "import neurite as ne\n",
    "import pandas as pd\n",
    "import random\n",
    "import nibabel as nb\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import Cardiac4DCT_Synth_Diffusion.Build_lists.Build_list as Build_list\n",
    "import Cardiac4DCT_Synth_Diffusion.functions_collection as ff\n",
    "import Cardiac4DCT_Synth_Diffusion.Data_processing as Data_processing\n",
    "import Cardiac4DCT_Synth_Diffusion.Generator_voxelmorph as Generator_voxelmorph\n",
    "main_path = '/mnt/camca_NAS/4DCT/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each case, the voxelmorph is optimized/trained individually. Then we apply the trained model to get MVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_name = 'voxel_morph_warp0_onecase'\n",
    "which_timeframe_is_template = 'others' # 'others' means warp 0 to other time frames, '0' means warp other time frames to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training for each case until converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Training ########\n",
    "\n",
    "## set patient list\n",
    "data_sheet = os.path.join(main_path, 'example_data/Patient_lists/example_data/patient_list.xlsx')\n",
    "\n",
    "b = Build_list.Build(data_sheet)\n",
    "patient_class_list, patient_id_list, tf_list = b.__build__(batch_list = [0]) \n",
    "\n",
    "results = []\n",
    "for i in range(0,1):\n",
    "    patient_class = patient_class_list[i]\n",
    "    patient_id = patient_id_list[i]\n",
    "    tf_num = tf_list[i]\n",
    "    \n",
    "    print(patient_class, patient_id, tf_num)\n",
    "\n",
    "    # set save path\n",
    "    save_path = os.path.join(main_path, 'example_data//models', trial_name, 'individual_models',patient_class,patient_id)\n",
    "    ff.make_folder([os.path.join(main_path,'example_data/models'), os.path.join(main_path,'example_data/models',trial_name), \n",
    "                    os.path.join(main_path,'example_data/models',trial_name, 'individual_models'), \n",
    "                    os.path.join(main_path,'example_data/models',trial_name, 'individual_models', patient_class), \n",
    "                    os.path.join(main_path,'example_data/models',trial_name, 'individual_models', patient_class, patient_id),\n",
    "                    os.path.join(save_path,'models'), \n",
    "                    os.path.join(save_path,'logs')])\n",
    "\n",
    "    # check whether the patient has been processed\n",
    "    if os.path.isfile(os.path.join(save_path,'models/vxm_final.h5')):\n",
    "        print('patient:', patient_id, 'has been processed')\n",
    "        continue\n",
    "\n",
    "    ## build the model\n",
    "    input_shape = [160,160,96]\n",
    "    nb_features = [[16, 32, 32, 32],[32, 32, 32, 32, 32, 16, 16]]\n",
    "    vxm_model = vxm.networks.VxmDense(input_shape, nb_features, int_steps=0)\n",
    "    losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss]\n",
    "    loss_weights = [1, 0.05]\n",
    "    vxm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights)\n",
    "\n",
    "    ## set the generator\n",
    "    train_generator = Generator_voxelmorph.DataGenerator_alltf(\n",
    "            np.asarray([patient_class]),\n",
    "            np.asarray([patient_id]),\n",
    "            np.asarray([tf_num]),\n",
    "            which_timeframe_is_template = which_timeframe_is_template,\n",
    "            main_path = '/mnt/camca_NAS/4DCT/',\n",
    "            patient_num = 1,\n",
    "            batch_size = 1,\n",
    "            shuffle = False,\n",
    "            normalize = True,\n",
    "            adapt_shape = [160,160,96],)\n",
    "    \n",
    "    # check whether there is a pre-trained model\n",
    "    pre_model_list = ff.find_all_target_files(['vxm_*'],os.path.join(save_path, 'models'))\n",
    "    pre_model_list = np.delete(pre_model_list, np.where(pre_model_list == os.path.join(save_path, 'models/vxm_final.h5'))) # if you want to re-train the model, you need to delete the previous final one\n",
    "  \n",
    "    if len(pre_model_list) == 0:\n",
    "        start_epoch = 0\n",
    "        print('no pre-trained model')\n",
    "    else:\n",
    "        pre_model_list = ff.sort_timeframe(pre_model_list,1,'_')\n",
    "        pre_model = pre_model_list[-1]\n",
    "        start_epoch = ff.find_timeframe(pre_model,1,'_')\n",
    "        vxm_model.load_weights(pre_model)\n",
    "        print('pre-trained model loaded, epoch:', start_epoch)\n",
    "\n",
    "    # ### train the model\n",
    "    nb_epochs = 1000\n",
    "\n",
    "    ### Initialize an Excel sheet data storage\n",
    "    loss_results = []\n",
    "\n",
    "    ### training loop\n",
    "    previous_loss = 100; freeze_count = 0\n",
    "    for epoch in range(start_epoch , start_epoch + nb_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{nb_epochs}\")\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        hist = vxm_model.fit(train_generator, epochs=1, verbose=1,use_multiprocessing=False,workers = 1, shuffle = False,)\n",
    "\n",
    "        # Get the training loss\n",
    "        training_loss = hist.history['loss'][0]\n",
    "        transformer_loss = hist.history.get('vxm_dense_transformer_loss', [None])[0]\n",
    "        flow_loss = hist.history.get('vxm_dense_flow_loss', [None])[0]\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # save the loss results\n",
    "            epoch_results = [epoch + 1, training_loss, transformer_loss, flow_loss]\n",
    "            loss_results.append(epoch_results)\n",
    "            df = pd.DataFrame(loss_results, columns=['Epoch', 'Training Loss', 'Transformer Loss', 'Flow Loss'])\n",
    "            file_name = os.path.join(save_path, 'logs/training_metrics.xlsx')\n",
    "            df.to_excel(file_name, index=False)\n",
    "\n",
    "            # Save the model parameters for each epoch\n",
    "            vxm_model.save(os.path.join(save_path,'models/vxm_'+str(epoch + 1)+'.h5'))\n",
    "\n",
    "            training_loss_round = round(training_loss, 4)\n",
    "            # check whether we should stop the training\n",
    "            if training_loss_round < previous_loss:\n",
    "                previous_loss = training_loss_round; freeze_count = 0\n",
    "            else:\n",
    "                freeze_count += 1\n",
    "\n",
    "            if epoch <= 150:\n",
    "                continue # at least train 150 epochs\n",
    "\n",
    "            if training_loss_round <= 0.0021 or epoch >= 300:\n",
    "                print('training loss is less than 0.0021 or epoch >= 300, stop at epoch:', epoch)\n",
    "                # copy and paste the last model to the final model\n",
    "                vxm_model.save(os.path.join(save_path,'models/vxm_final.h5'))\n",
    "                break\n",
    "            \n",
    "            if freeze_count >= 4: # 40 epochs no improvement\n",
    "                print('training loss has not improved for 40 epochs, stop at epoch:', epoch)\n",
    "                # copy and paste the last model to the final model\n",
    "                vxm_model.save(os.path.join(save_path,'models/vxm_final.h5'))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ground truth MVF for each case using trained Voxelmorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_data example_1 20\n",
      "example_data example_2 20\n"
     ]
    }
   ],
   "source": [
    "####### Testing ########\n",
    "# set the patient list\n",
    "data_sheet = os.path.join(main_path,'example_data/Patient_lists/example_data/patient_list.xlsx')\n",
    "\n",
    "b = Build_list.Build(data_sheet)\n",
    "patient_class_test_list, patient_id_test_list, tf_test_list = b.__build__(batch_list = [0]) \n",
    "\n",
    "for i in range(0, patient_id_test_list.shape[0]):\n",
    "    patient_class = patient_class_test_list[i]\n",
    "    patient_id = patient_id_test_list[i]\n",
    "    tf_num = tf_test_list[i]\n",
    "   \n",
    "    print(patient_class, patient_id, tf_num)\n",
    "\n",
    "    ### check whether the we have the voxel final model\n",
    "    model_path = os.path.join(main_path, 'example_data/models', trial_name, 'individual_models',patient_class,patient_id,'models/vxm_final.h5')\n",
    "    if not os.path.isfile(model_path):\n",
    "        print('no model for patient:', patient_id)\n",
    "        continue\n",
    "\n",
    "    ### set save path\n",
    "    save_path = os.path.join(main_path, 'example_data/mvf_warp0_onecase',patient_class,patient_id, 'voxel_final')\n",
    "    ff.make_folder([os.path.join(main_path,'example_data/mvf_warp0_onecase'), os.path.join(main_path,'example_data/mvf_warp0_onecase',patient_class),\n",
    "                    os.path.join(main_path,'example_data/mvf_warp0_onecase',patient_class,patient_id),\n",
    "                    save_path])\n",
    "\n",
    "    ### build the model\n",
    "    input_shape = [160,160,96]\n",
    "    nb_features = [[16, 32, 32, 32],[32, 32, 32, 32, 32, 16, 16]]\n",
    "    vxm_model = vxm.networks.VxmDense(input_shape, nb_features, int_steps=0)\n",
    "    # voxelmorph has a variety of custom loss classes\n",
    "    losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss]\n",
    "    loss_weights = [1, 0.05]\n",
    "    vxm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=losses, loss_weights=loss_weights)\n",
    "    vxm_model.load_weights(model_path)\n",
    "\n",
    "    ### do the prediction\n",
    "    image_path = os.path.join(main_path,'example_data/nii-images' ,patient_class, patient_id,'img-nii-resampled-1.5mm')\n",
    "    tf_files = ff.sort_timeframe(ff.find_all_target_files(['*.nii.gz'],image_path),2)\n",
    "\n",
    "    affine = nb.load(tf_files[0]).affine\n",
    "    image_shape = nb.load(tf_files[0]).shape\n",
    "    \n",
    "    for timeframe in range(0,len(tf_files)):\n",
    "        if os.path.isfile(os.path.join(save_path, str(timeframe) + '.nii.gz')) == 1:\n",
    "            print('timeframe:', timeframe, 'has been processed')\n",
    "        else:\n",
    "            original_image = nb.load(tf_files[timeframe]).get_fdata()\n",
    "            if timeframe == 0:\n",
    "                mvf = np.zeros([160,160,96,3]) \n",
    "                mae = 0\n",
    "                moved_pred = nb.load(tf_files[0]).get_fdata()\n",
    "                moved_pred = Data_processing.crop_or_pad(moved_pred, [160,160,96], value = np.min(original_image))\n",
    "            else:\n",
    "                if which_timeframe_is_template == 'others':\n",
    "                    tf1 = nb.load(tf_files[0]).get_fdata()\n",
    "                    tf2 = nb.load(tf_files[timeframe]).get_fdata()\n",
    "                else:\n",
    "                    tf1 = nb.load(tf_files[timeframe]).get_fdata()\n",
    "                    tf2 = nb.load(tf_files[0]).get_fdata()\n",
    "\n",
    "                if len(tf1.shape) == 4:\n",
    "                    tf1 = tf1[...,0]\n",
    "                if len(tf2.shape) == 4:\n",
    "                    tf2 = tf2[...,0]\n",
    "                \n",
    "                tf1 = Data_processing.crop_or_pad(tf1, [160,160,96], value = np.min(tf1)) / 1000\n",
    "                tf2 = Data_processing.crop_or_pad(tf2, [160,160,96], value = np.min(tf2)) / 1000\n",
    "\n",
    "              \n",
    "                val_input = [ tf1[np.newaxis, ..., np.newaxis],\n",
    "                    tf2[np.newaxis, ..., np.newaxis]]\n",
    "                    \n",
    "                val_pred = vxm_model.predict(val_input)\n",
    "                moved_pred = val_pred[0].squeeze() * 1000\n",
    "                pred_warp = val_pred[1]\n",
    "                mvf = pred_warp.squeeze()\n",
    "                \n",
    "            save_file = os.path.join(save_path, str(timeframe) + '.nii.gz')\n",
    "            img = nb.Nifti1Image(mvf, affine)\n",
    "            nb.save(img, save_file)\n",
    "\n",
    "            moved_pred_img = nb.Nifti1Image(moved_pred, affine)\n",
    "            nb.save(moved_pred_img, os.path.join(save_path, str(timeframe) + '_moved.nii.gz'))\n",
    "\n",
    "            original_image = Data_processing.crop_or_pad(original_image, [160,160,96], value = np.min(original_image))\n",
    "            nb.save(nb.Nifti1Image(original_image, affine), os.path.join(save_path, str(timeframe) + '_original.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
